{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2477c380-b115-4090-bdce-24ce7912466e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def reset(dest_path):\n",
    "    dbutils.fs.rm(dest_path, True)\n",
    "\n",
    "def copy_next_n_rows(\n",
    "  n,\n",
    "  source_table,\n",
    "  dest_path):\n",
    "    \n",
    "    df = spark.table(source_table)\n",
    "\n",
    "    # Find latest timestamp in destination\n",
    "    dest_exists = os.path.exists(dest_path)\n",
    "    dest_files = os.listdir(dest_path) if dest_exists else []\n",
    "    if dest_files:\n",
    "        dest_df = spark.read.json(dest_path)\n",
    "        if dest_df.count() > 0:\n",
    "            last_ts = dest_df.agg({\"timestamp\": \"max\"}).collect()[0][0]\n",
    "        else:\n",
    "            last_ts = None\n",
    "    else:\n",
    "        last_ts = None\n",
    "\n",
    "    # Filter for next N rows after last_ts\n",
    "    if last_ts:\n",
    "        next_rows = df.filter(df.timestamp > last_ts).orderBy(\"timestamp\").limit(n)\n",
    "    else:\n",
    "        next_rows = df.orderBy(\"timestamp\").limit(n)\n",
    "    \n",
    "    # If nothing to write, exit\n",
    "    if next_rows.count() == 0:\n",
    "        print(\"No new rows to copy.\")\n",
    "        return\n",
    "    \n",
    "    # Write as JSON, append mode, preserve format\n",
    "    next_rows.write.mode(\"append\").json(dest_path)\n",
    "    print(f\"Copied {next_rows.count()} rows to {dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d2eb277-1a59-467e-afe0-daccce65253f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dest_path = \"/Volumes/rtlh_lakehouse_labs/bootcamp_oct_2025/resources/data/gym_stream/\"\n",
    "source_table = \"rtlh_lakehouse_labs.bootcamp_oct_2025.lab2_bronze\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795d5ddb-4e62-46ea-872f-b22e08958efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#reset(dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b66fb9f-72bb-47b6-a4cb-d0997974e464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "  n = 1000\n",
    "  sleep = 10\n",
    "  copy_next_n_rows(n, source_table, dest_path)\n",
    "  print(f\"Copied {n} rows at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "  print(f\"Sleeping for {sleep} seconds...\")\n",
    "  time.sleep(sleep)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Instructor Only - Stage Incremental Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
